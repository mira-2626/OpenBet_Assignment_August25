{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3581287-14a7-43b3-b8fb-86f5f1b271d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e8b5202-ba36-40d3-ac91-3d44e6082a44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../utils/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7fd370f-37de-4ed1-be91-1d764e8e9f18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"run_date\", \"30072025\", \"Run Date\")\n",
    "run_date = dbutils.widgets.get(\"run_date\")\n",
    "print(f\"Running pipeline for date: {run_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "131611cd-4f1c-4e25-88af-2794e0a12213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load config\n",
    "current_dir = os.getcwd()\n",
    "config_path = os.path.join(current_dir, '..', '..', '..', 'config', 'config.yaml')\n",
    "config_path = os.path.normpath(config_path)\n",
    "config, root_path, master_data_folder, master_data_files = load_config(config_path)\n",
    "operational_folder = config['paths']['operational_data_folder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfefbdf6-9a94-4078-b078-58ce53a83af9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b02dbe8-f3f5-4331-b890-413d153f50a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_operational_data(config, run_date, root_path, operational_folder): \n",
    "    datasets = []\n",
    "\n",
    "    for file_key, file_info in config['operational_files'].items():\n",
    "        base_name = file_info['base_name']\n",
    "        extension = file_info['extension']\n",
    "        schema_config = file_info.get('schema')  # get schema config, may be None\n",
    "        spark_schema = parse_schema(schema_config) if schema_config else None\n",
    "\n",
    "        filename = f\"{base_name}_{run_date}.{extension}\"\n",
    "        file_path = os.path.join(root_path, operational_folder, filename)\n",
    "        print(f\"Loading {file_key} from {file_path}\")\n",
    "\n",
    "        print(f\"Schema config: {schema_config}\")\n",
    "        print(f\"Extension: {extension}\")\n",
    "\n",
    "        if extension.lower() == 'csv':\n",
    "            if spark_schema:\n",
    "                df = spark.read.schema(spark_schema).csv(file_path, header=True)\n",
    "            else:\n",
    "                df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "        elif extension.lower() == 'json':\n",
    "            if spark_schema:\n",
    "                print(\"Using schema for JSON read\")\n",
    "                df = spark.read.schema(spark_schema).json(file_path)\n",
    "            else:\n",
    "                df = spark.read.json(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file extension for {file_key}: {extension}\")\n",
    "\n",
    "        primary_key = file_info.get('primary_key', [])\n",
    "        if isinstance(primary_key, str):\n",
    "            primary_key = [primary_key]\n",
    "\n",
    "        datasets.append({\n",
    "            \"name\": file_key,\n",
    "            \"df\": df,\n",
    "            \"pk_cols\": primary_key,\n",
    "        })\n",
    "\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e449acb8-3d25-40e5-9caa-cffb5b219719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datasets = load_operational_data(config, run_date, root_path, operational_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6a55e33-7df3-4af3-b2b0-d6c1ad6ae6f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def validate_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Validates a dataset dictionary with keys:\n",
    "    - 'name': string, dataset name\n",
    "    - 'df': Spark DataFrame\n",
    "    - 'pk_cols': list of primary key column names\n",
    "    \"\"\"\n",
    "\n",
    "    name = dataset['name']\n",
    "    df = dataset['df']\n",
    "    pk_cols = dataset['pk_cols']\n",
    "    print(f\"Validating dataset: {name}\")\n",
    "\n",
    "    # Check if DataFrame is empty\n",
    "    check_empty(df, name)\n",
    "\n",
    "    # Check duplicates in primary key columns\n",
    "    check_duplicates(df, pk_cols, name)\n",
    "\n",
    "    # Check nulls in critical columns\n",
    "    check_empty(df, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3109699e-1093-4852-8af5-9d79edffbda0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TODO: de-hardcode value, move it to utils\n",
    "def flatten_team_moves(dataset):\n",
    "    if dataset['name'] == 'team_moves':\n",
    "        df = dataset['df']\n",
    "        exploded_df = df.select(\n",
    "            \"user_id\",\n",
    "            explode(\"team\").alias(\"team_move\")\n",
    "        ).select(\n",
    "            \"user_id\",\n",
    "            \"team_move.team_id\",\n",
    "            \"team_move.effective_from\"\n",
    "        )\n",
    "        dataset['df'] = exploded_df\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fcb7cb5-5fbd-4765-98eb-3a341f3cab52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datasets = [flatten_team_moves(ds) for ds in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8228f9e6-cc84-4d41-a305-e0c7f1c3a850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "\n",
    "    validate_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab063bf5-bdc8-4460-8dc3-4c480cfcbe46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#@TODO: Add audit columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14a111d3-3ed6-4dd1-9140-a079979c53d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fact_df = next(ds['df'] for ds in datasets if ds['name'] == 'fact_feed')\n",
    "fact_table = f\"{config['operational_layer']['schema']}.fact_feed\"\n",
    "team_moves_df = next(ds['df'] for ds in datasets if ds['name'] == 'team_moves')\n",
    "team_moves_table = f\"{config['operational_layer']['schema']}.team_moves\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5e1ba11-c717-4daa-aa84-4b016b95b45b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Fact feed rows: {fact_df.count() if fact_df else 'Not loaded'}\")\n",
    "print(f\"Team moves rows: {team_moves_df.count() if team_moves_df else 'Not loaded'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ff0ea72-7c8a-4f92-8858-a27908a78ad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def validate_fact_feed(df):\n",
    "    pk_cols = ['datekey', 'user_id', 'task_id']\n",
    "    critical_cols = pk_cols + ['recordedtimehours']\n",
    "    check_duplicates(df, pk_cols, 'fact_feed')\n",
    "    check_nulls(df, critical_cols, 'fact_feed')\n",
    "\n",
    "def validate_team_moves(df):\n",
    "    pk_cols = ['user_id', 'effective_from']\n",
    "    critical_cols = pk_cols + ['team_id']\n",
    "    check_duplicates(df, pk_cols, 'team_moves')\n",
    "    check_nulls(df, critical_cols, 'team_moves')\n",
    "\n",
    "validate_fact_feed(fact_df)\n",
    "validate_team_moves(team_moves_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcc79be0-94b1-434b-ae7e-b639d7e9d7ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Create schema and Delta table if it doesn't exist (empty table)\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS openbet_operational; --TODO: hardcode\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS openbet_operational.fact_feed ( --TODO: hardcode\n",
    "  datekey INT,\n",
    "  user_id INT,\n",
    "  task_id INT,\n",
    "  recordedtimehours DOUBLE\n",
    ")\n",
    "USING DELTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6c09ab6-bde3-4333-b2aa-252e7e6fe6c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merge_condition = \"\"\"\n",
    "  target.datekey = source.datekey AND\n",
    "  target.user_id = source.user_id AND\n",
    "  target.task_id = source.task_id\n",
    "\"\"\"\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "deltaTable = DeltaTable.forName(spark, fact_table)  \n",
    "\n",
    "# Perform merge\n",
    "(deltaTable.alias(\"target\")\n",
    "    .merge(fact_df.alias(\"source\"), merge_condition)\n",
    "    .whenMatchedUpdateAll()\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6544dcc1-b3dc-4416-b4c2-268446f9c871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Create schema and Delta table if it doesn't exist (empty table)\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS openbet_operational; --TODO: hardcode\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS openbet_operational.fact_feed ( --TODO: hardcode\n",
    "  datekey INT,\n",
    "  user_id INT,\n",
    "  task_id INT,\n",
    "  recordedtimehours DOUBLE\n",
    ")\n",
    "USING DELTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "013d414e-c2f1-475d-9d7c-e2b08fd746a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS openbet_operational;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS openbet_operational.team_moves (\n",
    "  user_id STRING,\n",
    "  team_id INT,\n",
    "  effective_from STRING\n",
    ")\n",
    "USING DELTA;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f364cdd9-b73e-4ba4-a3d8-dee5767f259c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merge_condition = \"\"\"\n",
    "  target.user_id = source.user_id AND\n",
    "  target.team_id = source.team_id AND\n",
    "  target.effective_from = source.effective_from\n",
    "\"\"\"\n",
    "\n",
    "team_moves_table = f\"{config['operational_layer']['schema']}.team_moves\"\n",
    "\n",
    "# Load Delta table for team_moves\n",
    "deltaTeamMoves = DeltaTable.forName(spark, team_moves_table)\n",
    "# Or use forPath if you prefer:\n",
    "# deltaTeamMoves = DeltaTable.forPath(spark, \"/path/to/team_moves/delta/table\")\n",
    "\n",
    "# Perform merge/upsert\n",
    "(\n",
    "    deltaTeamMoves.alias(\"target\")\n",
    "    .merge(team_moves_df.alias(\"source\"), merge_condition)\n",
    "    .whenMatchedUpdateAll()\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a6f4f23-5080-4c0d-9cf7-f8eb52588186",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# DROP TABLE IF EXISTS openbet_operational.fact_feed;\n",
    "\n",
    "# DROP TABLE IF EXISTS openbet_operational.team_moves;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b63e89c-f4bb-4631-b9ee-3b7d7c6859a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from openbet_operational.fact_feed;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4a7a9b7-e44b-4f0f-b917-d362a16c8a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from openbet_operational.team_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d26a96f0-e704-41c6-a49e-d19fc53d36ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: ADD postruns sanity checks"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5083073230368532,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "daily_ingestion.py",
   "widgets": {
    "run_date": {
     "currentValue": "01082025",
     "nuid": "a77fd428-8c5c-4049-8ee3-3b9b06815fc4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "30072025",
      "label": "Run Date",
      "name": "run_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "30072025",
      "label": "Run Date",
      "name": "run_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
