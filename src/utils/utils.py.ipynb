{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "785a28ef-1c69-4652-8596-5426265c8618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    IntegerType, StringType, ArrayType, BooleanType, DoubleType, DateType, TimestampType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b66c28e-6747-430b-8abf-122f9fe8dff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_config(config_path):\n",
    "    \"\"\"\n",
    "    Loads YAML config file and returns:\n",
    "    - full config dict\n",
    "    - root_path (str)\n",
    "    - master_data_folder (str)\n",
    "    - master_data_files (dict with nested filename and primary_key)\n",
    "    \"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(f\"Loaded config from {config_path}\")\n",
    "\n",
    "    root_path = config['paths']['root_path']\n",
    "    master_data_folder = config['paths']['master_data_folder']\n",
    "    master_data_files = config['master_data_files']\n",
    "\n",
    "    return config, root_path, master_data_folder, master_data_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7582c58d-f671-4770-97c7-831ed7d79472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_full_path(table_info, root_path, master_data_folder):\n",
    "    \"\"\"\n",
    "    Constructs the full file path for a given table_info dict containing 'filename',\n",
    "    using the root_path and master_data_folder from config.\n",
    "    \"\"\"    \n",
    "    filename = table_info['filename']\n",
    "    return os.path.join(root_path, master_data_folder, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2017ebb7-daef-4ac3-b01d-d63892ef9047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_duplicates(df, pk_col, table_name):\n",
    "    \"\"\"\n",
    "    Checks for duplicate primary keys in a DataFrame and raises an error if found.\n",
    "    \"\"\"\n",
    "    duplicates = df.groupBy(pk_col).agg(count(\"*\").alias(\"cnt\")).filter(col(\"cnt\") > 1)\n",
    "    if duplicates.count() > 0:\n",
    "        print(f\"Found duplicate(s) in {table_name} on {pk_col}:\")\n",
    "        duplicates.show(truncate=False)\n",
    "        df = df.dropDuplicates([primary_key])\n",
    "        print(f\"Dropped duplicates\") # we dont raise error for the shake of the exercise\n",
    "\n",
    "    else:\n",
    "        print(f\"[OK] No duplicates found in {table_name} on {pk_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6131bf1-42bb-41e3-8788-8fb874eec01f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_empty(df, table_name):\n",
    "    if len(df.head(1)) == 0: # erverless compute in Databricks does not support .rdd\n",
    "        raise ValueError(f\"[ERROR] Dataset '{table_name}' is empty!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5b80aa5-fbed-405b-b224-129c742a61c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_schema(schema_config):\n",
    "    \"\"\"\n",
    "    Converts a list of column specs from config into a Spark StructType\n",
    "    \"\"\"\n",
    "    type_map = {\n",
    "    \"IntegerType\": IntegerType(),\n",
    "    \"StringType\": StringType(),\n",
    "    \"DoubleType\": DoubleType(),\n",
    "    \"DateType\": DateType(),\n",
    "    \"TimestampType\": TimestampType(),\n",
    "    \"BooleanType\": BooleanType()\n",
    "    }\n",
    "\n",
    "    fields = []\n",
    "    for col_spec in schema_config:\n",
    "        col_name = col_spec['name']\n",
    "        col_type_str = col_spec['type']\n",
    "        nullable = col_spec.get('nullable', True)\n",
    "\n",
    "        if col_type_str == \"ArrayType\":\n",
    "            # Recursively parse elementType\n",
    "            element_schema_config = col_spec.get('elementType', [])\n",
    "            element_type = parse_schema(element_schema_config)\n",
    "            col_type = ArrayType(element_type, containsNull=nullable)\n",
    "        else:\n",
    "            col_type = type_map.get(col_type_str)\n",
    "            if col_type is None:\n",
    "                raise ValueError(f\"Unknown type {col_type_str} in schema config\")\n",
    "\n",
    "        fields.append(StructField(col_name, col_type, nullable))\n",
    "\n",
    "    return StructType(fields)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utils.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
